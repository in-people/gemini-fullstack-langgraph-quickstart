#!/usr/bin/env python3
"""
ä¸´æ—¶è¿è¡Œè„šæœ¬ï¼Œé¿å…ä¸site-packagesä¸­çš„agentæ¨¡å—å†²çª
å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæ—¶ä½¿ç”¨ï¼Œé¿å…ä¸site-packagesä¸­çš„agentæ¨¡å—å†²çª
åœ¨æœ¬åœ°è¿è¡Œæµ‹è¯•ï¼
"""
import sys
import os
import logging
from datetime import datetime
from dotenv import load_dotenv

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # åˆ›å»ºlogsç›®å½•
    log_dir = os.path.join(os.path.dirname(__file__), "logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"run_temp_{timestamp}.log")
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    return logging.getLogger(__name__)

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logging()

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))

# å°†é¡¹ç›®ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„çš„æœ€å‰é¢
sys.path.insert(0, current_dir)
sys.path.insert(0, project_root)

# åŠ è½½ç¯å¢ƒå˜é‡
logger.info("ğŸ“¥ å¼€å§‹åŠ è½½ç¯å¢ƒå˜é‡...")
load_dotenv(os.path.join(current_dir, ".env"))

# è®°å½•ç¯å¢ƒå˜é‡çŠ¶æ€
api_key_status = "âœ… å·²è®¾ç½®" if os.getenv("ALI_QWEN_API_KEY") else "âŒ æœªè®¾ç½®"
logger.info(f"ğŸ” ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ - ALI_QWEN_API_KEY: {api_key_status}")
logger.info(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
logger.debug(f"ğŸ“š Pythonè·¯å¾„å‰3ä¸ª: {sys.path[:3]}")

print("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
print("Pythonè·¯å¾„:", sys.path[:3])  # æ˜¾ç¤ºå‰3ä¸ªè·¯å¾„
print("ALI_QWEN_API_KEY:", os.getenv("ALI_QWEN_API_KEY")[:10] + "..." if os.getenv("ALI_QWEN_API_KEY") else "æœªè®¾ç½®")

# éªŒè¯ç¯å¢ƒå˜é‡
if not os.getenv("ALI_QWEN_API_KEY"):
    logger.error("âŒ ç¯å¢ƒå˜é‡ALI_QWEN_API_KEYæœªè®¾ç½®")
    print("âŒ ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    sys.exit(1)
else:
    logger.info("âœ… ç¯å¢ƒå˜é‡å·²è®¾ç½®")
    print("âœ… ç¯å¢ƒå˜é‡å·²è®¾ç½®")

# æ‰‹åŠ¨å¯¼å…¥é¡¹ç›®æ¨¡å—
logger.info("ğŸ“¦ å¼€å§‹å¯¼å…¥é¡¹ç›®æ¨¡å—...")
try:
    logger.debug("æ­£åœ¨å¯¼å…¥agent.tools_and_schemas...")
    from agent.tools_and_schemas import SearchQueryList, Reflection
    logger.debug("æ­£åœ¨å¯¼å…¥agent.state...")
    from agent.state import OverallState, QueryGenerationState, ReflectionState, WebSearchState
    logger.debug("æ­£åœ¨å¯¼å…¥agent.configuration...")
    from agent.configuration import Configuration
    logger.debug("æ­£åœ¨å¯¼å…¥agent.prompts...")
    from agent.prompts import get_current_date, query_writer_instructions, answer_instructions
    logger.debug("æ­£åœ¨å¯¼å…¥agent.utils...")
    from agent.utils import get_research_topic
    logger.debug("æ­£åœ¨å¯¼å…¥langchainå’Œlanggraphæ¨¡å—...")
    from langchain_core.messages import AIMessage
    from langgraph.types import Send
    from langgraph.graph import StateGraph, START, END
    from langchain_core.runnables import RunnableConfig
    from langchain_openai import ChatOpenAI
    
    logger.info("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
    
    # é‡æ–°å®šä¹‰éœ€è¦çš„å‡½æ•°
    def load_key(key_name: str):
        """Load API key from environment"""
        key_value = os.getenv(key_name)
        logger.debug(f"ğŸ”‘ åŠ è½½ç¯å¢ƒå˜é‡: {key_name} = {'*' * len(key_value) if key_value else 'None'}")
        return key_value
    
    # Initialize OpenAI client with Qwen model
    logger.info("ğŸ¤– åˆå§‹åŒ–Qwen Pluså®¢æˆ·ç«¯...")
    client = ChatOpenAI(
        temperature=0.0, 
        model="qwen-plus", 
        api_key=load_key("ALI_QWEN_API_KEY"), 
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    logger.info("âœ… Qwen Pluså®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    # é‡æ–°å®šä¹‰èŠ‚ç‚¹å‡½æ•°
    def generate_query(state: OverallState, config: RunnableConfig):
        logger.info("ğŸ” å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹")
        logger.info(f"ğŸ“ ç ”ç©¶ä¸»é¢˜: {get_research_topic(state['messages'])}")
        
        configurable = Configuration.from_runnable_config(config)
        logger.debug(f"âš™ï¸ é…ç½®ä¿¡æ¯: æ¨¡å‹={configurable.query_generator_model}, æŸ¥è¯¢æ•°é‡={configurable.number_of_initial_queries}")
        
        if state.get("initial_search_query_count") is None:
            state["initial_search_query_count"] = configurable.number_of_initial_queries
            logger.debug(f"ğŸ“Š è®¾ç½®åˆå§‹æŸ¥è¯¢æ•°é‡: {configurable.number_of_initial_queries}")
        
        logger.info("ğŸ¤– åˆå§‹åŒ–æŸ¥è¯¢ç”Ÿæˆæ¨¡å‹")
        llm = ChatOpenAI(
            model=configurable.query_generator_model,
            temperature=1.0,
            max_retries=2,
            api_key=load_key("ALI_QWEN_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        structured_llm = llm.with_structured_output(SearchQueryList)
        
        current_date = get_current_date()
        formatted_prompt = query_writer_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state["messages"]),
            number_queries=state["initial_search_query_count"],
        )
        logger.debug(f"ğŸ“„ æç¤ºè¯æ ¼å¼åŒ–å®Œæˆï¼ŒæŸ¥è¯¢æ•°é‡: {state['initial_search_query_count']}")
        
        logger.info("ğŸš€ è°ƒç”¨LLMç”Ÿæˆæœç´¢æŸ¥è¯¢...")
        result = structured_llm.invoke(formatted_prompt)
        
        logger.info(f"âœ… æŸ¥è¯¢ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(result.query)} ä¸ªæŸ¥è¯¢")
        for i, query in enumerate(result.query, 1):
            logger.info(f"   {i}. {query}")
        
        return {"search_query": result.query}
    
    def continue_to_web_research(state):
        return [
            Send("web_research", {"search_query": search_query, "id": int(idx)})
            for idx, search_query in enumerate(state["search_query"])
        ]
    
    def web_research(state: WebSearchState, config: RunnableConfig):
        logger.info("ğŸŒ å¼€å§‹æ‰§è¡Œç½‘ç»œç ”ç©¶èŠ‚ç‚¹")
        logger.info(f"ğŸ” ç ”ç©¶æŸ¥è¯¢: {state['search_query']}")
        
        configurable = Configuration.from_runnable_config(config)
        logger.debug(f"âš™ï¸ é…ç½®ä¿¡æ¯: æ¨¡å‹={configurable.query_generator_model}")
        
        logger.info("ğŸ“š ç”Ÿæˆç ”ç©¶æç¤ºè¯...")
        formatted_prompt = f"""You are a research assistant. Based on the following research topic, generate a comprehensive response with relevant information:

Research Topic: {state["search_query"]}

Provide detailed information that would typically come from web research. Structure your response with headings, key findings, and relevant data."""
        
        logger.info("ğŸš€ è°ƒç”¨LLMè¿›è¡Œç ”ç©¶å†…å®¹ç”Ÿæˆ...")
        response = client.invoke(formatted_prompt)
        
        logger.info(f"âœ… ç ”ç©¶å®Œæˆï¼Œç”Ÿæˆå†…å®¹é•¿åº¦: {len(response.content)} å­—ç¬¦")
        logger.debug(f"ğŸ“„ ç ”ç©¶å†…å®¹é¢„è§ˆ: {response.content[:200]}...")
        
        resolved_urls = {}
        sources_gathered = []
        logger.debug("ğŸ“‹ å½“å‰æœªä½¿ç”¨å®é™…ç½‘ç»œæœç´¢ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
        
        logger.info("ğŸ“¤ è¿”å›ç ”ç©¶ç»“æœ")
        return {
            "sources_gathered": sources_gathered,
            "search_query": [state["search_query"]],
            "web_research_result": [response.content],
        }
    
    def reflection(state: OverallState, config: RunnableConfig):
        logger.info("ğŸ§  å¼€å§‹æ‰§è¡Œåæ€èŠ‚ç‚¹")
        logger.info(f"ğŸ“Š å½“å‰ç ”ç©¶å¾ªç¯æ¬¡æ•°: {state.get('research_loop_count', 0) + 1}")
        
        configurable = Configuration.from_runnable_config(config)
        state["research_loop_count"] = state.get("research_loop_count", 0) + 1
        reasoning_model = state.get("reasoning_model", configurable.reflection_model)
        logger.debug(f"âš™ï¸ é…ç½®ä¿¡æ¯: åæ€æ¨¡å‹={reasoning_model}")
        
        current_date = get_current_date()
        # æ„å»ºæç¤ºè¯
        research_results = '\n\n---\n\n'.join(state["web_research_result"])
        logger.debug(f"ğŸ“„ åæ€æç¤ºè¯æ ¼å¼åŒ–å®Œæˆï¼Œç ”ç©¶ç»“æœæ•°é‡: {len(state['web_research_result'])}")
        
        formatted_prompt = f"""Based on the research results below, analyze if the information is sufficient to answer the original question. If there are knowledge gaps, suggest follow-up queries.

Current Date: {current_date}
Research Topic: {get_research_topic(state["messages"])}
Research Results: {research_results}

Respond in JSON format with:
- is_sufficient: boolean
- knowledge_gap: string describing what's missing
- follow_up_queries: list of 1-2 follow-up search queries"""
        
        logger.info("ğŸ¤– åˆå§‹åŒ–åæ€æ¨¡å‹")
        llm = ChatOpenAI(
            model=reasoning_model,
            temperature=1.0,
            max_retries=2,
            api_key=load_key("ALI_QWEN_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        
        logger.info("ğŸš€ è°ƒç”¨LLMè¿›è¡Œåæ€åˆ†æ...")
        result = llm.invoke(formatted_prompt)
        
        # ç®€å•è§£æç»“æœï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼‰
        content = result.content.lower()
        is_sufficient = "true" in content or "sufficient" in content
        
        logger.info(f"âœ… åæ€åˆ†æå®Œæˆ")
        logger.info(f"   çŸ¥è¯†æ˜¯å¦å……åˆ†: {'æ˜¯' if is_sufficient else 'å¦'}")
        knowledge_gap = "éœ€è¦æ›´å¤šè¯¦ç»†ä¿¡æ¯" if not is_sufficient else "ä¿¡æ¯å……åˆ†"
        logger.info(f"   çŸ¥è¯†ç¼ºå£: {knowledge_gap}")
        follow_up_queries = ["è¡¥å……ä¿¡æ¯æŸ¥è¯¢"] if not is_sufficient else []
        logger.info(f"   åç»­æŸ¥è¯¢æ•°é‡: {len(follow_up_queries)}")
        
        for i, query in enumerate(follow_up_queries, 1):
            logger.info(f"   åç»­æŸ¥è¯¢ {i}: {query}")
        
        return {
            "is_sufficient": is_sufficient,
            "knowledge_gap": knowledge_gap,
            "follow_up_queries": follow_up_queries,
            "research_loop_count": state["research_loop_count"],
            "number_of_ran_queries": len(state["search_query"]),
        }
    
    def evaluate_research(state, config: RunnableConfig):
        configurable = Configuration.from_runnable_config(config)
        max_research_loops = (
            state.get("max_research_loops")
            if state.get("max_research_loops") is not None
            else configurable.max_research_loops
        )
        if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
            return "finalize_answer"
        else:
            return [
                Send(
                    "web_research",
                    {
                        "search_query": follow_up_query,
                        "id": state["number_of_ran_queries"] + int(idx),
                    },
                )
                for idx, follow_up_query in enumerate(state["follow_up_queries"])
            ]
    
    def finalize_answer(state: OverallState, config: RunnableConfig):
        logger.info("ğŸ¯ å¼€å§‹æ‰§è¡Œç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹")
        logger.info(f"ğŸ“Š ç ”ç©¶ç»“æœæ•°é‡: {len(state['web_research_result'])}")
        
        configurable = Configuration.from_runnable_config(config)
        reasoning_model = state.get("reasoning_model") or configurable.answer_model
        logger.debug(f"âš™ï¸ é…ç½®ä¿¡æ¯: ç­”æ¡ˆæ¨¡å‹={reasoning_model}")
        
        logger.info("ğŸ“š æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆæç¤ºè¯...")
        current_date = get_current_date()
        formatted_prompt = answer_instructions.format(
            current_date=current_date,
            research_topic=get_research_topic(state["messages"]),
            summaries="\n---\n\n".join(state["web_research_result"]),
        )
        logger.debug(f"ğŸ“„ ç­”æ¡ˆæç¤ºè¯æ ¼å¼åŒ–å®Œæˆï¼Œç ”ç©¶ç»“æœé•¿åº¦: {len(state['web_research_result'])}")
        
        logger.info("ğŸ¤– åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆæ¨¡å‹")
        llm = ChatOpenAI(
            model=reasoning_model,
            temperature=0,
            max_retries=2,
            api_key=load_key("ALI_QWEN_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        logger.info("ğŸš€ è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        result = llm.invoke(formatted_prompt)
        
        logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")
        logger.debug(f"ğŸ“„ ç­”æ¡ˆå†…å®¹é¢„è§ˆ: {result.content[:200]}...")
        
        logger.info("ğŸ“¤ è¿”å›æœ€ç»ˆç­”æ¡ˆ")
        return {
            "messages": [AIMessage(content=result.content)],
            "sources_gathered": state["sources_gathered"],
        }
    
    # åˆ›å»ºgraph
    logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºLangGraphæµç¨‹å›¾...")
    builder = StateGraph(OverallState, config_schema=Configuration)
    
    logger.debug("æ·»åŠ èŠ‚ç‚¹: generate_query, web_research, reflection, finalize_answer")
    builder.add_node("generate_query", generate_query)
    builder.add_node("web_research", web_research)
    builder.add_node("reflection", reflection)
    builder.add_node("finalize_answer", finalize_answer)
    
    logger.debug("è®¾ç½®å›¾çš„è¿æ¥å…³ç³»...")
    builder.add_edge(START, "generate_query")
    builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])
    builder.add_edge("web_research", "reflection")
    builder.add_conditional_edges("reflection", evaluate_research, ["web_research", "finalize_answer"])
    builder.add_edge("finalize_answer", END)
    
    logger.info("ğŸ”„ ç¼–è¯‘LangGraph...")
    graph = builder.compile(name="pro-search-agent")
    
    logger.info("âœ… Graphåˆ›å»ºæˆåŠŸ")
    print("âœ… Graphåˆ›å»ºæˆåŠŸ")
    
    # è¿è¡Œæµ‹è¯•
    logger.info("=" * 60)
    logger.info("ğŸš€ LangGraph ç ”ç©¶åŠ©æ‰‹å¯åŠ¨ (ä¸´æ—¶è¿è¡Œè„šæœ¬)")
    logger.info("=" * 60)
    
    query = "å·´è²ç‰¹æŠ•èµ„ç†å¿µï¼ä¸€å®šä½¿ç”¨ä¸­æ–‡å›ç­”"
    logger.info(f"ğŸ“¥ æ¥æ”¶åˆ°ç”¨æˆ·æŸ¥è¯¢: {query}")
    
    initial_state = {
        "messages": [{"role": "user", "content": query}]
    }
    
    logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œç ”ç©¶æµç¨‹...")
    start_time = datetime.now()
    
    try:
        result = graph.invoke(initial_state)
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("âœ… ç ”ç©¶æµç¨‹æ‰§è¡Œå®Œæˆ")
        logger.info(f"â±ï¸  æ‰§è¡Œè€—æ—¶: {duration:.2f} ç§’")
        
        # æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ¯ æœ€ç»ˆç ”ç©¶ç»“æœ")
        print("=" * 60)
        print(result["messages"][-1].content)
        print("=" * 60)
        
        logger.info("ğŸ“ ç»“æœå·²è¾“å‡ºåˆ°æ§åˆ¶å°")
        logger.info("ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ° logs/ ç›®å½•")
        
    except Exception as e:
        logger.error(f"âŒ ç ”ç©¶æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        logger.info("=" * 60)
        logger.info("ğŸ”š LangGraph ç ”ç©¶åŠ©æ‰‹æ‰§è¡Œç»“æŸ")
        logger.info("=" * 60)
    
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()